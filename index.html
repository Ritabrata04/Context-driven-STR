<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">

  <!-- ─── Basic & Social Metadata ─────────────────────────────────────────────── -->
  <meta name="description"
        content="We introduce a training‑free, context‑driven pipeline that segments and recognises scene text with far lower compute than current end‑to‑end systems, yet matches state‑of‑the‑art accuracy on ICDAR 13/15 and Total‑Text.">
  <meta property="og:title"
        content="A Lightweight Context‑Driven Training‑Free Network for Scene Text Segmentation &amp; Recognition">
  <meta property="og:description"
        content="ICDAR 2025 paper ‑ We leverage background cues and plug‑and‑play recognisers to skip heavy detection, achieving SOTA accuracy with 60 % fewer FLOPs.">
  <meta property="og:url"           content="https://your‑site.org/ContextDrivenSTR">
  <meta property="og:image"         content="static/images/banner.png">
  <meta property="og:image:width"   content="1200">
  <meta property="og:image:height"  content="630">

  <meta name="twitter:title"
        content="Context‑Driven STR ‑ training‑free scene text spotting (ICDAR 2025)">
  <meta name="twitter:description"
        content="A lightweight framework that uses background context to recognise scene text while skipping costly detection.">
  <meta name="twitter:image"        content="static/images/banner_twitter.png">
  <meta name="twitter:card"         content="summary_large_image">

  <!-- Keywords for indexing -->
  <meta name="keywords"
        content="scene text recognition, scene text segmentation, training‑free, context‑aware, lightweight models, ICDAR 2025, computer vision">

  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Context‑Driven STR | ICDAR 2025</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">

  <!-- ─── Fonts & CSS ─────────────────────────────────────────────────────────── -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <!-- ─── JS ──────────────────────────────────────────────────────────────────── -->
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>

<!-- ──────────────────────────── Hero ────────────────────────────────────────── -->
<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">
            A Lightweight Context‑Driven Training‑Free Network for<br>
            Scene Text Segmentation&nbsp;and&nbsp;Recognition
          </h1>

          <!-- Authors -->
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=xxxxxxxx" target="_blank">
                Ritabrata Chakraborty</a><sup>*</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=yyyyyyyy" target="_blank">
                Shivakumara Palaiahnakote</a><sup>*</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=zzzzzzzz" target="_blank">
                Umapada Pal</a>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=aaaaaaaa" target="_blank">
                Cheng‑Lin Liu</a>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">
              Indian Statistical Institute Kolkata&nbsp;&nbsp;·&nbsp;
              Manipal University Jaipur&nbsp;&nbsp;·&nbsp;
              University of Salford&nbsp;&nbsp;·&nbsp;
              UCAS, Beijing
              <br>20<sup>th</sup> ICDAR — International Conference on Document Analysis and
              Recognition, 2025
            </span>
            <br><span class="eql-cntrb">
              <small><sup>*</sup>Equal contribution</small>
            </span>
          </div>

          <!-- Links -->
          <div class="publication-links">
            <span class="link-block">
              <a href="https://arxiv.org/pdf/2407.01234.pdf" target="_blank"
                 class="external-link button is-normal is-rounded is-dark">
                <span class="icon"><i class="fas fa-file-pdf"></i></span><span>Paper</span>
              </a>
            </span>

            <span class="link-block">
              <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                 class="external-link button is-normal is-rounded is-dark">
                <span class="icon"><i class="fas fa-file-pdf"></i></span>
                <span>Supplementary</span>
              </a>
            </span>

            <span class="link-block">
              <a href="https://github.com/YourUser/ContextDrivenSTR" target="_blank"
                 class="external-link button is-normal is-rounded is-dark">
                <span class="icon"><i class="fab fa-github"></i></span><span>Code</span>
              </a>
            </span>

            <span class="link-block">
              <a href="https://arxiv.org/abs/2407.01234" target="_blank"
                 class="external-link button is-normal is-rounded is-dark">
                <span class="icon"><i class="ai ai-arxiv"></i></span><span>arXiv</span>
              </a>
            </span>
          </div><!-- /links -->

        </div>
      </div>
    </div>
  </div>
</section>

<!-- ───────────────────────── Teaser video ───────────────────────────────────── -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="static/images/teaser_frame.png" id="teaser" autoplay controls muted loop>
        <source src="static/videos/banner_video.mp4" type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        <em>Skip the detector, keep the accuracy:</em> our pipeline uses a lightweight
        attention‑gated U‑Net, block‑level cropping and background context to deliver
        state‑of‑the‑art scene‑text accuracy with 60 % fewer FLOPs.
      </h2>
    </div>
  </div>
</section>

<!-- ───────────────────────── Abstract ───────────────────────────────────────── -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Modern scene‑text recognition (STR) systems typically rely on bulky,
            end‑to‑end detectors and recognisers that demand extensive training
            and incur prohibitive latency in real‑time settings.  We present a
            <strong>training‑free, plug‑and‑play framework</strong> that exploits scene context
            to bypass heavy detection altogether.  An attention‑gated U‑Net refines
            candidate text regions at the pixel level; simple block‑level cropping
            produces text patches for a lightweight recogniser (e.g.&nbsp;PARSeq‑S).
            Simultaneously, a BLIP‑2 captioner distils background cues that help
            verify or reject predictions via semantic and lexical similarity.  Only
            uncertain cases fall back to a full STR model (DeepSolo).  On
            ICDAR‑13/15 and Total‑Text we match or exceed state‑of‑the‑art
            accuracy while saving up to 60 % FLOPs and memory, demonstrating
            that <em>context‑driven STR</em> is both effective and efficient.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- ───────────────────────── Image carousel ─────────────────────────────────── -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item">
          <img src="static/images/qual_ic13.jpg" alt="Recognition on ICDAR‑13">
          <h2 class="subtitle has-text-centered">ICDAR‑13&nbsp;— focused scene text</h2>
        </div>
        <div class="item">
          <img src="static/images/qual_ic15.jpg" alt="Recognition on ICDAR‑15">
          <h2 class="subtitle has-text-centered">ICDAR‑15&nbsp;— incidental scene text</h2>
        </div>
        <div class="item">
          <img src="static/images/qual_totaltext.jpg" alt="Recognition on Total‑Text">
          <h2 class="subtitle has-text-centered">Total‑Text&nbsp;— curved text</h2>
        </div>
        <div class="item">
          <img src="static/images/pipeline.jpg" alt="Pipeline overview">
          <h2 class="subtitle has-text-centered">Pipeline overview</h2>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- ───────────────────────── Video presentation ─────────────────────────────── -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Video&nbsp;Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="publication-video">
            <iframe src="https://www.youtube.com/embed/JkaxUblCGz0"
                    frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>
            </iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- ───────────────────────── Poster ─────────────────────────────────────────── -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>
      <iframe src="static/pdfs/poster.pdf" width="100%" height="550"></iframe>
    </div>
  </div>
</section>

<!-- ───────────────────────── BibTeX ─────────────────────────────────────────── -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
<pre><code>@inproceedings{chakraborty2025contextstr,
  title     = {A Lightweight Context-Driven Training-Free Network for Scene Text Segmentation and Recognition},
  author    = {Chakraborty, Ritabrata and Palaiahnakote, Shivakumara and Pal, Umapada and Liu, Cheng-Lin},
  booktitle = {Proc. International Conference on Document Analysis and Recognition (ICDAR)},
  year      = {2025}
}</code></pre>
  </div>
</section>

<!-- ───────────────────────── Footer ─────────────────────────────────────────── -->
<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This page is built from the
            <a href="https://github.com/eliahuhorwitz/Academic-project-page-template"
               target="_blank">Academic&nbsp;Project&nbsp;Page&nbsp;Template</a>,
            adapted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project.
            Source licensed under
            <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"
               target="_blank">CC&nbsp;BY‑SA&nbsp;4.0</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
